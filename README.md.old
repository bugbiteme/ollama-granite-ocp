# ollama-granite-ocp
Demo for running Granite LLM on OpenShift with Ollama and accessing it via Open WebUI

Notes:
- Demo was built on OpenShift running on AWS.
- If you are running this in a non-AWS environment, or have a storage class, other than `gp3-csi`, update the PVC resource definitions
-  Tested on single node OpenShift on an `mx6a.4xlarge` EC2 instance and is a resource hog (espsecially memory)  
- To change the LLM being used, update `MODEL_NAME` in `config-ollama.yaml` (`granite3.3:8b` is the current defualt value) 


## To deploy in OpenShift
To deploy in OpenShift using Kustomize, run the command:

```bash
oc apply -k k8/base  
```

This will deploy the ollama stack with a single model (`granite3.3:8b` is the current defualt value) 

To deploy multi-model ollama stack, use Kustomize to deploy the `mutli-model` demo
```bash
oc apply -k k8/overlays/multi-model
```

This will load:
- `granite3.3:8b`
- `mistral`
- `llama3`

into the `ollama` deployment running in the `multi-model` namespace. The models to be loaded are set in `config-ollama.yanml`.
  
Once deployed, run the command:
```bash
oc get routes   
```

to get to the Open WebUI interface URL

example:
```bash
oc get routes                                                                             
NAME         HOST/PORT
open-webui   open-webui-ollama-granite-demo.apps.<domain>
```

When you go to the URL for the first time, the web page will ask you to create an admin account to log in.

